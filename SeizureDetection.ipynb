{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeizureDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOtn0TKjocJc9tx6PxVcdfg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josephmhernandez/SeizureDetection/blob/master/SeizureDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMZXK42oRwUB",
        "colab_type": "text"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuG0FiRFR7BI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading openPose.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcZlUrmWS1a8",
        "colab_type": "text"
      },
      "source": [
        "## openPose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PxEJKYWS36G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Open Pose\n",
        "import os\n",
        "from os.path import exists, join, basename, splitext\n",
        "\n",
        "git_repo_url = 'https://github.com/CMU-Perceptual-Computing-Lab/openpose.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "if not exists(project_name):\n",
        "  # see: https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/949\n",
        "  # install new CMake becaue of CUDA10\n",
        "  !wget -q https://cmake.org/files/v3.13/cmake-3.13.0-Linux-x86_64.tar.gz\n",
        "  !tar xfz cmake-3.13.0-Linux-x86_64.tar.gz --strip-components=1 -C /usr/local\n",
        "  # clone openpose\n",
        "  !git clone -q --depth 1 $git_repo_url\n",
        "  !sed -i 's/execute_process(COMMAND git checkout master WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\\/3rdparty\\/caffe)/execute_process(COMMAND git checkout f019d0dfe86f49d1140961f8c7dec22130c83154 WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\\/3rdparty\\/caffe)/g' openpose/CMakeLists.txt\n",
        "  # install system dependencies\n",
        "  !apt-get -qq install -y libatlas-base-dev libprotobuf-dev libleveldb-dev libsnappy-dev libhdf5-serial-dev protobuf-compiler libgflags-dev libgoogle-glog-dev liblmdb-dev opencl-headers ocl-icd-opencl-dev libviennacl-dev\n",
        "  # install python dependencies\n",
        "  !pip install -q youtube-dl\n",
        "  # build openpose\n",
        "  !cd openpose && rm -rf build || true && mkdir build && cd build && cmake .. && make -j`nproc`\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydg6-2DmS9-o",
        "colab_type": "text"
      },
      "source": [
        "### Creating action landmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU_hyToeTBlw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Connect to Google Drive. \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijZmL1CsTMih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Seizure Videos openPose output generation. \n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "data = 'gdrive/My\\ Drive/Seizure\\ DL/Dataset/SeizureVideos/Vids/'\n",
        "typeVid = 'Seizure'\n",
        "jdest = 'gdrive/My\\ Drive/Seizure\\ DL/Dataset/SeizureVideos/Data/'\n",
        "\n",
        "pythonPushupsPath = 'gdrive/My Drive/Seizure DL/Dataset/SeizureVideos/'\n",
        "\n",
        "list_vids = np.array(os.listdir(pythonPushupsPath + 'Vids/'))\n",
        "i = 1\n",
        "err_list = []\n",
        "for filename in list_vids:\n",
        "  try:\n",
        "    typeVid = filename[:len(filename) - 4]\n",
        "    \n",
        "    newFolder = pythonPushupsPath + typeVid + \"vidOutput\" + str(i)\n",
        "\n",
        "    newvidInput = data + filename\n",
        "    newvidOutput = jdest + typeVid + \"vidOutput\" + str(i) + \".avi\"\n",
        "    newJsonOutput = jdest + typeVid + \"vidOutput\" + str(i) + '/'\n",
        "\n",
        "    !./openpose/build/examples/openpose/openpose.bin --video {newvidInput} --write_video {newvidOutput} --write_json {newJsonOutput} --model_folder openpose/models --disable_blending --display 0 --number_people_max 1 --scale_number 4 --scale_gap 0.25 --keypoint_scale 3\n",
        "\n",
        "  except: \n",
        "    err_list.append(filename)\n",
        "  i += 1\n",
        "\n",
        "print(err_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKvCuqBkTf3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Organizing videos and generating openPose output. \n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "actions = ['Jump', 'Laugh', 'PushUps', 'Sit', 'Situp']\n",
        "# actions = ['Sit', 'Situp'] 'Brush_hair', 'Clap', 'Climbing_stairs', 'Drink', 'Eat', \n",
        "# actions = ['Clap']\n",
        "\n",
        "for a in actions:\n",
        "  typeVid = a\n",
        "  data = 'gdrive/My\\ Drive/Seizure\\ DL/Dataset/NonSeizureVideos/' + typeVid + '/Vids/'\n",
        "  jdest = 'gdrive/My\\ Drive/Seizure\\ DL/Dataset/NonSeizureVideos/' + typeVid + '/Data/'\n",
        "\n",
        "\n",
        "  pythonPushupsPath = 'gdrive/My Drive/Seizure DL/Dataset/NonSeizureVideos/' + typeVid + '/' \n",
        "\n",
        "  try:\n",
        "    list_vids = np.array(os.listdir(pythonPushupsPath + 'Vids/'))\n",
        "  except:\n",
        "    print('bad Action:' + a)\n",
        "    \n",
        "  i = 1\n",
        "  err_list = []\n",
        "  for filename in list_vids:\n",
        "    try:\n",
        "      src = pythonPushupsPath + 'Vids/'+ filename\n",
        "      \n",
        "      newFolder = pythonPushupsPath + typeVid + \"vidOutput\" + str(i)\n",
        "\n",
        "      newvidInput = data + typeVid + \"vid\" +str(i) + \".avi\"\n",
        "      newvidOutput = jdest + typeVid + \"vidOutput\" + str(i) + \".avi\"\n",
        "      newJsonOutput = jdest + typeVid + \"vidOutput\" + str(i) + '/'\n",
        "\n",
        "      !./openpose/build/examples/openpose/openpose.bin --video {newvidInput} --write_video {newvidOutput} --write_json {newJsonOutput} --model_folder openpose/models --disable_blending --display 0 --number_people_max 1 --scale_number 4 --scale_gap 0.25 --keypoint_scale 3\n",
        "\n",
        "    except: \n",
        "      err_list.append(filename)\n",
        "    i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9QqO20PT_Kt",
        "colab_type": "text"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbR5KxdLUPFe",
        "colab_type": "text"
      },
      "source": [
        "## Data Formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH7cxBRlUYk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Produce (80, 50) np arrays and store them into npy documents. One document per each action  so ~200 documents. \n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "\n",
        "#Mount google drive. \n",
        "#Connect to Google Drive. \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "root_path = 'gdrive/My Drive/Seizure DL/Data_2/'\n",
        "\n",
        "send_path = 'gdrive/My Drive/Seizure DL/Data_3/'\n",
        "\n",
        "#Get three folders that need to be itterated through (Train, Test, Valid) \n",
        "folders = ['Validation', 'Train', 'Test']\n",
        "#Get list of all actions\n",
        "\n",
        "#Iterate through all actions (directories)\n",
        "count = 0\n",
        "for rootFolderName in folders: #[Train, test, validation]\n",
        "    rootFolderName = 'Test'\n",
        "    path = root_path + rootFolderName + '/'\n",
        "    actions = np.array(os.listdir(path))\n",
        "    print('start ' + rootFolderName)\n",
        "    for a in actions:\n",
        "\n",
        "        path2 = path + a + '/'\n",
        "        jsonFiles = np.array(os.listdir(path2))\n",
        "        rtnList = np.zeros((80,50)) \n",
        "        jsonFiles.sort()\n",
        "        for i,jsonFile in enumerate(jsonFiles): \n",
        "            with open(path2 + jsonFiles[i]) as jsonOutput:\n",
        "                data = json.load(jsonOutput)\n",
        "                try:\n",
        "                    bodyLandmarks = data['people'][0]['pose_keypoints_2d']\n",
        "                    #Get rid of the confidence of points\n",
        "                    bodyLandmarks = [xx for i, xx in enumerate(bodyLandmarks) if i%3 !=2]\n",
        "                    rtnList[i,] = bodyLandmarks\n",
        "                except:\n",
        "                    continue\n",
        "            #Only extracts the first 80 frames/time_steps\n",
        "            if (i > 78):\n",
        "                break\n",
        "        \n",
        "        #Convert (80, 50) np.array to a .npy file\n",
        "        np.save(send_path + rootFolderName + '/' + a + '.npy', rtnList)\n",
        "        count += 1 \n",
        "        if count % 5 == 0:\n",
        "            print(count)\n",
        "\n",
        "    print('Finished ' + rootFolderName)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsgsrcCXUY5Y",
        "colab_type": "text"
      },
      "source": [
        "## Function Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrfndpcZUdnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Allows multiprocessing during training. \n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "import keras\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, list_IDs, labels, location=\"location__\", batch_size=32, dim=(32,32,32), n_channels=1,\n",
        "                 n_classes=10, shuffle=True):\n",
        "        #'Initialization'\n",
        "\n",
        "        #(number of channels, number of classes, batch size)\n",
        "        self.dim = dim\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        #Dictionary whwere labels[ID] = label\n",
        "        self.labels = labels\n",
        "        \n",
        "        #List of \n",
        "        self.list_IDs = list_IDs\n",
        "\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.location_data = location\n",
        "\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        #Updates indexes after each epoch\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        X = np.empty((self.batch_size, *self.dim))#, self.n_channels))\n",
        "        y = np.empty((self.batch_size), dtype=int)\n",
        "\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "            # data = np.load(self.location + ID + ) \n",
        "            X[i,] = np.load(self.location_data + ID) \n",
        "           \n",
        "            # strInput = self.location_data + '/' + ID + '/'\n",
        "            # # print('strInput: ', strInput)\n",
        "            # X[i,] = self.getLandmarks(strInput).reshape(self.dim[0], self.dim[1])#, self.n_channels)\n",
        "\n",
        "            # Store class\n",
        "            y[i] = self.labels[ID]\n",
        "            # print(y.shape)\n",
        "        return X, y\n",
        "\n",
        "        # return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
        "\n",
        "    def __len__(self):\n",
        "        #'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def getLandmarks(self, pyPath):\n",
        "        #Input -> Location of json files (openPose output).\n",
        "        #Return -> list of the first 80 outputs in the JSON folders with 75 body landmarks. \n",
        "        rtnList = np.zeros((80,50))\n",
        "        jsonFiles = np.array(os.listdir(pyPath))\n",
        "        # print('jsonFiles List:', jsonFiles)\n",
        "        jsonFiles.sort()\n",
        "\n",
        "        for i,fileName in enumerate(jsonFiles): \n",
        "            with open(pyPath + jsonFiles[i]) as jsonOutput:\n",
        "                data = json.load(jsonOutput)\n",
        "                try:\n",
        "                    bodyLandmarks = data['people'][0]['pose_keypoints_2d']\n",
        "                    #Get rid of the confidence of points\n",
        "                    bodyLandmarks = [xx for i, xx in enumerate(bodyLandmarks) if i%3 !=2]\n",
        "                    rtnList[i] = bodyLandmarks\n",
        "                except:\n",
        "                    continue\n",
        "            #Only extracts the first 80 frames/time_steps\n",
        "            if (i > 78):\n",
        "                break\n",
        "\n",
        "        #Should return (frames = 80, landmarks/features = 75)\n",
        "        return rtnList"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLfSnN6_Uh0O",
        "colab_type": "text"
      },
      "source": [
        "## Editing Sparse Matricies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcCEd8z7UlN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random \n",
        "import os \n",
        "\n",
        "#For no more tensor flow warnings. \n",
        "from tensorflow.python.util import deprecation\n",
        "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
        "\n",
        "#Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "def fill_sparse_data(read_path, write_path):\n",
        "    #Some of the openPose outputs don't capture 'continuous' data for each landmark.\n",
        "    #The idea for solving this is to replace empty and sparse frames with more \n",
        "    #   continuous frame data. For example let X be an array of size 80 (80 timestamps)\n",
        "    #   X = [0, 0, 0, 1, 2, 3, 0, 0, 0, 3, 3, 4, 0, 5, 0, 0, 0, ....] \n",
        "    #   I will assume that these landmarks haven't gone off screen because there\n",
        "    #   coordinates for thes landmarks as the index increases. \n",
        "    #   Outputted feature vector:\n",
        "    #   new_X = \n",
        "    #       [0, 0, 0, 1, 2, 3, 3, 3, 3, 3, 3, 4, 4, 5, 5, 5, 5, ....] \n",
        "    # Note: Maybe would rather format end of the arrays to that there won't be \n",
        "    #       incorrect values if the landmark is off the screen and doesn't come\n",
        "    #       back. \n",
        "    #       Example:\n",
        "    #   new_new_X:\n",
        "    #       [0, 0, 0, 1, 2, 3, 3, 3, 3, 3, 3, 4, 4, 5, 0, 0, 0, .... , 0] \n",
        "    #   ... To keep the integrity of the features if the landmark leaves the screen at the end. \n",
        "\n",
        "    #Foramt read/write paths.\n",
        "    if(read_path[len(read_path) - 1] != '/'):\n",
        "        read_path += '/'\n",
        "    if(write_path[len(write_path) - 1] != '/'):\n",
        "        write_path += '/'\n",
        "    \n",
        "    files = os.listdir(path)\n",
        "    \n",
        "    for file_name in files: \n",
        "        data = np.load(path + file_name)\n",
        "        rtnData = np.zeros((len(data), 50))\n",
        "\n",
        "        _prevValues = np.zeros(50)       \n",
        "        for j,features in enumerate(data):\n",
        "            _values = np.zeros(50)\n",
        "            for i,f in enumerate(features): \n",
        "                #Iterate over 50 values. \n",
        "                if f == 0:\n",
        "                    _values[i] = _prevValues[i]\n",
        "                else:\n",
        "                    #f != 0:\n",
        "                    _values[i] = f\n",
        "                    _prevValues[i] = f\n",
        "\n",
        "            rtnData[j,] = _values\n",
        "\n",
        "\n",
        "        np.save(write_path + file_name, rtnData)\n",
        "        \n",
        "    print('Finished writing.')\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcX_AS5sVp47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Show the sparsity in openPose landmark featuers using Data_3 files. \n",
        "import numpy as np\n",
        "import os \n",
        "\n",
        "py_train_path = 'gdrive/My Drive/Seizure DL/All_Data_Non_Sparse/Train/'\n",
        "# py_test_path = 'gdrive/My Drive/Seizure DL/Data_3/Test/'\n",
        "# py_validation_path = 'gdrive/My Drive/Seizure DL/Data_3/Validation/'\n",
        "\n",
        "partition = {}\n",
        "partition['train'] = np.array(os.listdir(py_train_path))\n",
        "# partition['test'] = np.array(os.listdir(py_test_path))\n",
        "# partition['validation'] = np.array(os.listdir(py_validation_path))\n",
        "\n",
        "print('Percent of all zero features (each of the 50 features are 0) out of 80 timestamps:')\n",
        "for p in partition:\n",
        "    print(p + ' data:')\n",
        "\n",
        "    #Just to get the root path correct for each partition.\n",
        "    _temp_path = None\n",
        "    if (p is 'train'):\n",
        "        _temp_path = py_train_path\n",
        "    elif(p is 'test'):\n",
        "        _temp_path = py_test_path\n",
        "    else:\n",
        "        _temp_path = py_validation_path\n",
        "\n",
        "    for doc in partition[p]:\n",
        "        count_time_stamps = 0\n",
        "        zero_sum = 0 \n",
        "\n",
        "        _data = np.load(_temp_path + doc)\n",
        "\n",
        "        for time_stamp in _data:\n",
        "            count_time_stamps += 1\n",
        "            if(sum(time_stamp) == 0):\n",
        "                zero_sum += 1\n",
        "\n",
        "        print(doc + ' -> ' + str(zero_sum / count_time_stamps * 100))\n",
        "    print('\\n\\n') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAYbp0HyUm9Z",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IROW7gkVuvA",
        "colab_type": "text"
      },
      "source": [
        "## Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwh2RVpvVwav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define metrics to analyze training/testing/validataion\n",
        "#Build the model function. \n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "#Build the model function. \n",
        "from keras import models\n",
        "from keras import layers\n",
        "import keras_metrics as km\n",
        "import keras.backend as K\n",
        "\n",
        "def build_model_2():\n",
        "    seq_model = models.Sequential()\n",
        "    seq_model.add(layers.LSTM(200, activation='relu', input_shape=(80, 50), return_sequences = False))\n",
        "    # seq_model.add(layers.LSTM(40, activation='relu', return_sequences = False))\n",
        "    seq_model.add(layers.Dropout(.2))\n",
        "    seq_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.5, nesterov=True)\n",
        "\n",
        "    seq_model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy', f1_score])\n",
        "\n",
        "    return seq_model\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def build_model_3():\n",
        "    seq_model = models.Sequential()\n",
        "    seq_model.add(layers.LSTM(1000, activation='relu', input_shape=(80, 50), return_sequences = False))\n",
        "    # seq_model.add(layers.LSTM(40, activation='relu', return_sequences = False))\n",
        "    seq_model.add(layers.Dropout(.35))\n",
        "    seq_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.3, nesterov=True)\n",
        "\n",
        "    seq_model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy', f1_score, precision_m, recall_m])\n",
        "\n",
        "    return seq_model\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "#Define F1 score. \n",
        "def f1_score(y_true, y_pred):\n",
        "\n",
        "    # Count positive samples.\n",
        "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "\n",
        "    # If there are no true samples, fix the F1 score at 0.\n",
        "    if c3 == 0:\n",
        "        return 0\n",
        "\n",
        "    # How many selected items are relevant?\n",
        "    precision = c1 / c2\n",
        "\n",
        "    # How many relevant items are selected?\n",
        "    recall = c1 / c3\n",
        "\n",
        "    # Calculate f1_score\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj3BY5WzWVNq",
        "colab_type": "text"
      },
      "source": [
        "## K-Fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrH5c4OMWa6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random \n",
        "import os \n",
        "\n",
        "#For no more tensor flow warnings. \n",
        "from tensorflow.python.util import deprecation\n",
        "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
        "\n",
        "#Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsrrOZFHrVu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Build the model function. \n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "import random\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "#Hyper-parameters \n",
        "k = 4 \n",
        "batch_size = 16\n",
        "num_epochs = 150\n",
        "\n",
        "\n",
        "#Create dictionary entries of all data. \n",
        "py_train_path = 'gdrive/My Drive/Seizure DL/OverSample/Train/'\n",
        "\n",
        "partition = {}\n",
        "partition['train'] = np.array(os.listdir(py_train_path))\n",
        "\n",
        "\n",
        "labels = {}\n",
        "for p in partition:\n",
        "    for item in partition[p]:\n",
        "        if 'seizure' in item:\n",
        "            labels[item] = 1\n",
        "        else:\n",
        "            labels[item] = 0\n",
        "\n",
        "\n",
        "params ={'location': py_train_path,\n",
        "        'dim': (80,50),\n",
        "          'batch_size': batch_size,\n",
        "          'n_classes': 1,\n",
        "          'n_channels': 1,\n",
        "          'shuffle': True}\n",
        "\n",
        "validation_params ={'location': py_train_path,\n",
        "        'dim': (80,50),\n",
        "          'batch_size': 62,\n",
        "          'n_classes': 1,\n",
        "          'n_channels': 1,\n",
        "          'shuffle': True}\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "#Training at a rediculuously large number of epochs so we can plot overfitting. \n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_f1_histories = []\n",
        "\n",
        "num_val_samples = len(partition['train']) // k \n",
        "print(num_val_samples)\n",
        "\n",
        "random.shuffle(partition['train'])\n",
        "\n",
        "model_histories = []\n",
        "save_models = []\n",
        "save_data = [] #[ (train_data, val_data), ... ]\n",
        "\n",
        "\n",
        "for i in range(k): \n",
        "    #Partition entire dataset into training and testing\n",
        "    val_data = partition['train'][i*num_val_samples: (i+1) * num_val_samples]\n",
        "    print('length of validation data:' + str(len(val_data)))\n",
        "    partial_train_data = np.concatenate([partition['train'][:i*num_val_samples], partition['train'][(i+1)*num_val_samples:]], axis=0)\n",
        "\n",
        "    save_data.append((partial_train_data,val_data))\n",
        "\n",
        "    #Create data generators for the train and test set.\n",
        "    training_generator = DataGenerator(partial_train_data, labels, **params)\n",
        "    validating_generator = DataGenerator(val_data, labels, **validation_params)\n",
        "    \n",
        "    model = build_model_2()\n",
        "    model.summary()\n",
        "\n",
        "    root = 'gdrive/My Drive/Seizure DL/OverSample/'\n",
        "\n",
        "    mc = ModelCheckpoint(filepath=root +'weights.{epoch:02d}.hdf5', monitor='val_loss', save_best_only=True)\n",
        "    callbacks = [mc]\n",
        "\n",
        "    history = model.fit_generator(generator=training_generator, validation_data=validating_generator, validation_freq = 10, use_multiprocessing=True, workers=4, epochs=num_epochs, callbacks = callbacks, verbose=1)\n",
        "    model_histories.append(history)\n",
        "    save_models.append(model)\n",
        "\n",
        "    acc_history = history.history['acc']\n",
        "    loss_history = history.history['loss']\n",
        "    f1_history = history.history['f1_score'] \n",
        "    \n",
        "    all_loss_histories.append(loss_history)\n",
        "    all_acc_histories.append(acc_history)\n",
        "    all_f1_histories.append(f1_history)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XthCJg3EW_u8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Evaluate model.\n",
        "path = 'gdrive/My Drive/Seizure DL/OverSample/Test/'\n",
        "\n",
        "(test_x, test_y) = load_data(path)\n",
        "\n",
        "pred = save_models[3].predict(test_x)\n",
        "save_models[3]\n",
        "for i,p in enumerate(pred):\n",
        "    print(str(p) + ' : ' + str(test_y[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOa_NjHTXMoz",
        "colab_type": "text"
      },
      "source": [
        "## Fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpI-inUCXQZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random \n",
        "import os \n",
        "\n",
        "#For no more tensor flow warnings. \n",
        "from tensorflow.python.util import deprecation\n",
        "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
        "\n",
        "#Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLYI7ERiXhMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train model with oversampled dataset. \n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import optimizers\n",
        "\n",
        "#HyperParameters\n",
        "batch_size = 16\n",
        "num_epochs = 450\n",
        "\n",
        "\n",
        "py_path = 'gdrive/My Drive/Seizure DL/OverSample_2/Train/'\n",
        "py_path_test = 'gdrive/My Drive/Seizure DL/OverSample_2/Test/'\n",
        "\n",
        "partition = {}\n",
        "partition['train'] = np.array(os.listdir(py_path))\n",
        "partition['test'] = np.array(os.listdir(py_path_test))\n",
        "\n",
        "\n",
        "#Debug. \n",
        "print('Length of training set: ' + str(len(partition['train'])))\n",
        "# print('Length of testing set: ' + str(len(partition['test'])))\n",
        "\n",
        "labels = {}\n",
        "for p in partition:\n",
        "    for item in partition[p]:\n",
        "        if 'seizure' in item:\n",
        "            labels[item] = 1\n",
        "        else:\n",
        "            labels[item] = 0\n",
        "\n",
        "_count = 0\n",
        "for item in partition['train']:\n",
        "    if 'seizure' in item:\n",
        "        _count += 1\n",
        "print('Number of Seizure videos in training set: ' + str(_count) + ', ' + str(_count / len(partition['train'])) + '%')\n",
        "\n",
        "random.shuffle(partition['train'])\n",
        "\n",
        "#Split up training for testing and validation. \n",
        "_size = len(partition['train']) // 8\n",
        "\n",
        "train_data = partition['train']\n",
        "validation_data = partition['test']\n",
        "test_data = partition['test']\n",
        "\n",
        "#Diagnostics on training and testing sets. \n",
        "s_count = 0\n",
        "for td in train_data:\n",
        "    s_count += labels[td]\n",
        "\n",
        "test_count = 0\n",
        "for td in test_data:\n",
        "    test_count+= labels[td]\n",
        "\n",
        "valid_count = 0\n",
        "for td in validation_data:\n",
        "    valid_count += labels[td]\n",
        "\n",
        "print('Training Data:\\n' + 'Total Videos: ' + str(len(train_data)) + '\\nSeizure Videos: ' + str(s_count))\n",
        "print('\\nValidation Data:\\n' + 'Total Videos: ' + str(len(validation_data)) + '\\nSeizure Videos: ' + str(valid_count))\n",
        "print('\\nTesting Data:\\n' + 'Total Videos: ' + str(len(test_data)) + '\\nSeizure Videos: ' + str(test_count))\n",
        "\n",
        "params ={'location': py_path,\n",
        "        'dim': (80,50),\n",
        "          'batch_size': batch_size,\n",
        "          'n_classes': 1,\n",
        "          'n_channels': 1,\n",
        "          'shuffle': True}\n",
        "\n",
        "test_params ={'location': py_path,\n",
        "        'dim': (80,50),\n",
        "          'batch_size': len(test_data),\n",
        "          'n_classes': 1,\n",
        "          'n_channels': 1,\n",
        "          'shuffle': True}\n",
        "\n",
        "validation_params ={'location': py_path_test,\n",
        "        'dim': (80,50),\n",
        "          'batch_size': len(validation_data),\n",
        "          'n_classes': 1,\n",
        "          'n_channels': 1,\n",
        "          'shuffle': True}\n",
        "\n",
        "training_generator = DataGenerator(train_data, labels, **params)\n",
        "\n",
        "# testing_generator = DataGenerator(test_data, labels, **test_params)\n",
        "\n",
        "validating_generator = DataGenerator(validation_data, labels, **validation_params)\n",
        "\n",
        "root = 'gdrive/My Drive/Seizure DL/OverSample_2/'\n",
        "\n",
        "#Build Model\n",
        "model = build_model_3()\n",
        "model.summary()\n",
        "#from keras.callbacks import ModelCheckpoint\n",
        "# mc = ModelCheckpoint(filepath=root +'weights.{epoch:02d}.hdf5', monitor='val_recall_m', mode='max', save_best_only=True)\n",
        "mc_f1 = ModelCheckpoint(filepath=root +'weights.{epoch:02d}.hdf5', monitor='val_loss', mode='min', save_best_only=True)\n",
        "# es = EarlyStopping(monitor='val_loss', baseline=5, verbose=1)\n",
        "call_backs = [mc_f1]\n",
        "#Fit model. \n",
        "history = model.fit_generator(generator=training_generator, validation_data=validating_generator, callbacks=call_backs, use_multiprocessing=True, workers=8, epochs=num_epochs, verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5diojYFJXnhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Print model.\n",
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='simple_model.png')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qu6vRpqpXrwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def smooth_curve(points, factor=.9):\n",
        "  smoothed_points = []\n",
        "  for point in points: \n",
        "    #print(point)\n",
        "    if smoothed_points: \n",
        "      previous = smoothed_points[-1]\n",
        "      #print(previous)\n",
        "      smoothed_points.append(previous*factor + point * (1-factor))\n",
        "    else:\n",
        "      smoothed_points.append(point)\n",
        "    \n",
        "  return smoothed_points\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure()\n",
        "val_loss = smooth_curve(history.history['val_loss'])\n",
        "loss = smooth_curve(history.history['loss'])\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "num_epochs = range(345)\n",
        "\n",
        "plt.plot(num_epochs, val_loss[:len(num_epochs)], 'r', label = 'Val Loss')\n",
        "plt.plot( num_epochs, loss[:len(num_epochs)], 'b', label = 'Train Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "val_loss = smooth_curve(history.history['val_precision_m'])\n",
        "loss = smooth_curve(history.history['precision_m'])\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Precision')\n",
        "num_epochs = range(450)\n",
        "\n",
        "\n",
        "plt.plot(num_epochs, val_loss[:len(num_epochs)], 'r', label = 'Val Precision')\n",
        "plt.plot( num_epochs, loss[:len(num_epochs)], 'b', label = 'Train Precision')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1qKBSfzXwXB",
        "colab_type": "text"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK2fLL9HXyJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random \n",
        "import os \n",
        "\n",
        "#For no more tensor flow warnings. \n",
        "from tensorflow.python.util import deprecation\n",
        "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
        "\n",
        "#Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "from keras import models\n",
        "\n",
        "#Load past model and evaluate the model\n",
        "model_path = 'gdrive/My Drive/Seizure DL/OverSample_2/weights.18.hdf5'\n",
        "train_data_path = 'gdrive/My Drive/Seizure DL/OverSample_2/Train/'\n",
        "test_data_path = 'gdrive/My Drive/Seizure DL/OverSample_2/Test/'\n",
        "\n",
        "#Load model \n",
        "dependencies = {\n",
        "    'f1_score': f1_score,\n",
        "    'precision_m': precision_m,\n",
        "    'recall_m': recall_m\n",
        "}\n",
        "# model = keras.models.load_model(self.output_directory + 'best_model.hdf5', custom_objects=dependencies)\n",
        "root = 'gdrive/My Drive/Seizure DL/OverSample_2/'\n",
        "# model_paths = ['weights.161.hdf5','weights.162.hdf5','weights.211.hdf5']\n",
        "model_paths = ['weights.340.hdf5']\n",
        "\n",
        "for mp in model_paths:\n",
        "    print('\\n')\n",
        "    print(mp)\n",
        "    # my_model = build_model_3()\n",
        "    my_model = load_model(root+mp, custom_objects=dependencies)\n",
        "    #Load testing data\n",
        "    (train_x, train_y, train_names), (test_x, test_y, test_names) = load_data(train_location=train_data_path, test_location=test_data_path, get_name=True)\n",
        "\n",
        "\n",
        "    h = my_model.evaluate(test_x, test_y, verbose=1)\n",
        "\n",
        "    for i in range(len(test_y)):\n",
        "        pred = my_model.predict(x=test_x[i].reshape(1,80,50))\n",
        "        print(test_names[i] + ' : ' + str(pred))\n",
        "\n",
        "    print('test output:')\n",
        "    print(h)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}